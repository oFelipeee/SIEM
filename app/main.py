# -*- coding: utf-8 -*-
from fastapi import FastAPI, UploadFile, File
import os
import shutil
import glob
import json
import logging

# Configura√ß√£o de logging para depura√ß√£o
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = FastAPI()

# Defini√ß√£o dos diret√≥rios
LOGS_DIR = "logs"
OUTPUT_DIR = "output"
UPLOAD_DIR = "uploaded_logs_temp"

# Garante que os diret√≥rios necess√°rios existem
def garantir_pasta(pasta: str):
    """Cria o diret√≥rio se ele n√£o existir."""
    if not os.path.exists(pasta):
        os.makedirs(pasta)
        logging.info(f"Diret√≥rio criado: {pasta}")

# Salva a sa√≠da da an√°lise em um arquivo JSON
def salvar_json(dados: list, filepath: str):
    """Salva os dados de an√°lise em um arquivo JSON."""
    try:
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(dados, f, indent=4, ensure_ascii=False)
        logging.info(f"An√°lise salva em: {filepath}")
    except IOError as e:
        logging.error(f"Erro ao salvar o arquivo JSON {filepath}: {e}")

# Fun√ß√£o central para an√°lise de uma √∫nica linha de log
def analisar_linha(linha: str):
    """
    Classifica eventos de log conforme risco, com justificativa e mitiga√ß√£o
    """
    linha_lower = linha.lower()

    if "failed password" in linha_lower or "authentication failure" in linha_lower:
        return {
            "evento": linha.strip(),
            "risco": "Alto",
            "justificativa": "Tentativa de acesso n√£o autorizado detectada.",
            "acao_mitigadora": "Habilitar bloqueio ap√≥s m√∫ltiplas tentativas e revisar acessos."
        }
    elif "connection refused" in linha_lower or "timeout" in linha_lower:
        return {
            "evento": linha.strip(),
            "risco": "M√©dio",
            "justificativa": "Falha de conex√£o pode indicar scanner de rede ou instabilidade.",
            "acao_mitigadora": "Verificar firewall, IDS/IPS e estabilidade do servi√ßo."
        }
    elif "error" in linha_lower or "warn" in linha_lower:
        return {
            "evento": linha.strip(),
            "risco": "Baixo",
            "justificativa": "Erro gen√©rico identificado no log.",
            "acao_mitigadora": "Revisar logs de aplica√ß√£o e corrigir configura√ß√£o."
        }
    else:
        return {
            "evento": linha.strip(),
            "risco": "Informativo",
            "justificativa": "Sem ind√≠cios de risco relevante.",
            "acao_mitigadora": "Nenhuma a√ß√£o necess√°ria."
        }

def processar_logs_do_diretorio():
    """Processa todos os arquivos de log nos diret√≥rios 'logs' e 'uploaded_logs_temp'."""
    garantir_pasta(LOGS_DIR)
    garantir_pasta(OUTPUT_DIR)

    arquivos = glob.glob(os.path.join(LOGS_DIR, "*.log")) + \
               glob.glob(os.path.join(LOGS_DIR, "*.txt"))
    
    # Adiciona arquivos tempor√°rios rec√©m-carregados
    arquivos += glob.glob(os.path.join(UPLOAD_DIR, "*.log")) + \
               glob.glob(os.path.join(UPLOAD_DIR, "*.txt"))

    resultados_totais = {}
    if not arquivos:
        return {"processados": "Nenhum arquivo de log encontrado para processar."}

    for arquivo in arquivos:
        nome_arquivo = os.path.basename(arquivo)
        resultados = []
        try:
            with open(arquivo, "r", encoding="utf-8", errors="ignore") as f:
                for linha in f:
                    if linha.strip():
                        resultados.append(analisar_linha(linha))
        except IOError as e:
            logging.error(f"Erro ao ler o arquivo {nome_arquivo}: {e}")
            continue

        saida_json_path = os.path.join(OUTPUT_DIR, f"{os.path.splitext(nome_arquivo)[0]}.json")
        salvar_json(resultados, saida_json_path)
        resultados_totais[nome_arquivo] = saida_json_path
    
    # Move os arquivos tempor√°rios para o diret√≥rio de logs principal ap√≥s o processamento
    for arquivo_temp in glob.glob(os.path.join(UPLOAD_DIR, "*")):
        shutil.move(arquivo_temp, os.path.join(LOGS_DIR, os.path.basename(arquivo_temp)))
        logging.info(f"Arquivo movido de '{UPLOAD_DIR}' para '{LOGS_DIR}': {os.path.basename(arquivo_temp)}")

    return {"processados": resultados_totais}


# Endpoint raiz
@app.get("/")
def root():
    """Mensagem de boas-vindas para o Mini-SIEM."""
    return {"message": "Mini-SIEM rodando üöÄ"}

# Endpoint para processar logs em lote
@app.get("/analisar")
def analisar_logs():
    """
    Processa todos os arquivos .log e .txt presentes no diret√≥rio 'logs/'
    e gera um relat√≥rio JSON para cada um.
    """
    logging.info("Iniciando a an√°lise de todos os logs existentes.")
    return processar_logs_do_diretorio()

# Endpoint para upload de um arquivo de log
@app.post("/upload_log")
async def upload_log(file: UploadFile = File(...)):
    """
    Recebe um arquivo de log via upload, o salva, analisa e retorna a an√°lise.
    O arquivo de log original √© salvo para processamento futuro.
    """
    garantir_pasta(UPLOAD_DIR)
    
    # Salva o arquivo temporariamente para processamento
    filepath_temp = os.path.join(UPLOAD_DIR, file.filename)
    try:
        with open(filepath_temp, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        logging.info(f"Arquivo temporariamente salvo: {filepath_temp}")
    except IOError as e:
        return {"erro": f"Erro ao salvar o arquivo: {e}"}

    # Processa o arquivo rec√©m-carregado
    resultados = processar_logs_do_diretorio()

    return {
        "arquivo_recebido": file.filename,
        "saida_analise": resultados.get("processados", {}).get(file.filename),
        "total_linhas": len(resultados.get("processados", {}).get(file.filename, []))
    }

if __name__ == "__main__":
    import uvicorn
    garantir_pasta(LOGS_DIR)
    garantir_pasta(OUTPUT_DIR)
    garantir_pasta(UPLOAD_DIR)
    logging.info("üîç Iniciando o Mini-SIEM...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
